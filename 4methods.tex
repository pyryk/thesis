%!TEX root = thesis.tex

\chapter{Approach and Methods}
\label{chapter:methods}

In this chapter, we define the research approach used for finding the answers to our research questions. Additionally, we present several methods for evaluating the case properties needed for the research approach, and of those, we pick the most suitable.

\emph{For this research, we employ both quantitative and qualitative research methods.} The evaluation of process efficiency (RQ1) is measured quantitatively, examining the numeric characteristics of several case studies. The visualization effectiveness (RQ2) is evaluated qualitatively, analyzing whether the tool encourages the visualizer to conform to visualization guidelines.

\section{Research Approach}

\emph{In order to find answers to the research questions\footnote{How does a reusable software system affect the \emph{efficiency} of building geographical visualizations?}\footnote{How does a reusable software system affect the \emph{effectiveness} of geographical visualizations?}, we decided to conduct a constructive study by implementing and evaluating a reusable geovisualization tool.} Constructive research excels at finding answers to questions of type ``how useful is system X'' \citep{jarvinen_tutkimustyon_2012}, making it the most suitable research type for this thesis.

In practice, conducting a constructive study involves designing an artifact and evaluating its effect \citep{jarvinen_tutkimustyon_2012}. \emph{Therefore, we decided to build a reusable geographical visualization tool}, evaluating its effect on the effectiveness of the visualization and the efficiency of building the visualization. This can be done by performing a \emph{case study}, i.e. observing one or more visualization cases and evaluating the relevant properties in those cases. In this study, we selected 6 visualization cases to evaluate the effect of the implemented tool on effectiveness and efficiency of visualization. The cases were: 1) store map depicting the locations of Alko stores, 2) map of earthquakes in California after January 1, 1900, 3) regional voter turnout in the Finnish presidential election of 2012, 4) regional share of people with no secondary education in Finland, 5) travel times to a single destination in Helsinki metropolitan area, and 6) average travel times to multiple destinations in Helsinki metropolitan area.

\section{Evaluating Software Reuse Effectiveness}

In section \ref{section:successfullreusefactors}, we concluded that it is critical to analyze and measure software reuse. Several methods for analyzing software reuse exist. \citet{frakes_software_1996} present six general types of software reuse analysis models: \emph{cost-benefit analysis} models, \emph{maturity assessment} models, \emph{amount-of-reuse} models, \emph{failure modes analysis}, \emph{reusability assessment} models and \emph{reuse library metrics}. 

Cost-benefit analysis models consider both development costs for the reusable component and the reuse productivity and quality benefits. Maturity assessment models categorize the matureness of a systematic software reuse program. Amount of reuse metrics assess the proportion of reused software in the software system created. Software reuse failure modes model introduces a set of reuse failures which are then used to assess a systematic reuse program. Reusability assessment models aim to analyze various software attributes to assess the reusability of a piece of code. Reuse library metrics concentrate on the reusability of software component libraries instead of single components.

Not all the models discussed above are applicable to this type of research. For instance, as maturity assessment models assess a reuse program as a whole instead of single reusable piece of software, it can not be used for this type of work. \emph{Therefore, we decided to use the cost-benefit analysis as it is the most suitable for assessing a single piece of software}.

The models presented above are high-level analysis tools which do not take a stance on how to measure the detailed data required by the measurements. Hence, the models need to be complemented with lower-level, more detail-oriented methods. In their review of multiple studies, \citet{mohagheghi_quality_2007} list several methods for software reuse analysis. Of them, notable ones include \emph{controlled experiments}, \emph{case studies}, \emph{surveys} and \emph{experience reports}. As the scope of this work does not allow for large sample sizes required by controlled experiment method, it is not a feasible alternative for this study. Moreover, using experience reports requires a considerable experience on creating and using reusable software which is not possible to achieve for this work. Therefore, we decided to use the case study method.

According to \citet{kitchenham_evaluating_1998}, one method for conducting a software project case study is using a \emph{sister project} comparison. In sister project comparison, a minimum of two different, but sufficiently similar software projects are observed. In at least one of the projects, a new method is employed, while in at least one project, the old method is in use. All other practices and aspects should be left unchanged. \citet{mohagheghi_quality_2007} argue that this kind of comparison is applicable for analyzing software reuse effectiveness for a specific piece of software. The sister project case study is appropriate when no systematic reuse program exists or when there are other barriers impeding the use of other models presented by \citet{mohagheghi_quality_2007}. Moreover, it is possible to create the sister project synthetically by building the same kind of application twice, once with and once without the reusable component. \emph{Due to these reasons, we decided to employ the sister project method for evaluation}, building multiple visualization cases separately with and without the tool.

As presented above, in the higher abstraction level, analyzing projects is fairly straightforward. However, the actual low-level measurements for reusing software are much more complex. \citet{mohagheghi_quality_2007} argue that measuring software reuse effectiveness precisely is difficult due to the following factors: 1) Metrics are difficult to validate since there is no universally accepted definition of ``quality'' in software products, and 2) the productivity of development is difficult to measure. Therefore, highly subjective, vague or even erroneous metrics are utilized.

Both \citet{frakes_software_1996} and \citet{mohagheghi_quality_2007} agree that the size of the code needed to be written correlates inversely to the development productivity. However, according to \citet{fenton_software_1998}, the number of lines in program source code is only one perspective for program size. It can be complemented by measuring the functionality and complexity of the program. Moreover, research by \citet{banker_software_1993,gill_cyclomatic_1991} show that software code complexity correlates inversely to the software maintenance productivity. \emph{Therefore, it seems evident that in order to enable productive use of reusable software, the new code to be written (outside the reusable components) should be made simple and concise.}

The conciseness of the code can be measured by several different metrics. According to \citet{fenton_software_1998}, the most commonly used metric for program size is its length, i.e., the number of lines of source code. The metric can be refined by only considering effective lines, ignoring lines consisting of comments and whitespace. However, \citet{fenton_software_1998} encourage the use of both effective and physical line counts to determine the size of a program. \emph{Therefore, we decided to use both of the measures for efficiency evaluation.}

The functionality of the program can be determined with several different metrics. \citet{fenton_software_1998} present the \emph{function point} approach, which uses the number and complexity of external inputs and outputs, \emph{object point} approach which uses the number of different screens and reports involved in the application, and so-called \emph{bang metrics} which use the total number of primitives in the data-flow diagram of the program. It should be noted that all of these methods are highly subjective and only provide speculative metrics.

Also the complexity of the program can be determined with several different techniques. According to \citet{fenton_software_1998}, the complexity of the program (i.e., solution to a problem) should ideally be not higher than the problem complexity. According to them, in the ideal case it is possible to determine the complexity of the program by determining the complexity of the problem. However, this is not usually the case in real-life applications. \citet{mccabe_complexity_1976} presents a computational approach for determining the complexity of an application implementation. His \emph{cyclomatic complexity} method involves counting the number of cycles in the program flow graph to approximate the program complexity. The advantage of this approach when compared to the method presented by \citet{fenton_software_1998} is that it can be used to compute complexity differences of multiple implementations of the same problem. \emph{Therefore, we decided to complement the line counts with the cyclomatic complexity method for determining the program complexity and the effort needed.}

In addition to the metrics presented above, \citet{halstead_elements_1977} presents a number of complexity measures. These measures can be used to estimate software size, difficulty level and the needed effort solely based on the code. The methods involve determining the number of operators (e.g., function calls) and operands (e.g., variables) in the program source code. These properties are then used to to approximate the higher-level software properties. Unlike the metrics of \citeauthor{mccabe_complexity_1976} or \citeauthor{fenton_software_1998}, \citeauthor{halstead_elements_1977} metrics include an explicit measure for development effort. This makes the metrics especially beneficial for our purpose. \emph{Therefore, we decided to include the \citeauthor{halstead_elements_1977} metrics for effort and difficulty to refine the collection of metrics for efficiency evaluation.}

Another approach for estimating program size is using the COnstructive COst MOdel (COCOMO) \citep{boehm_software_1981}. While COCOMO is most typically used for beforehand software project cost estimation, it can also be used to estimate effort. However, COCOMO and its variants are either too vague (COCOMO 81) or too process-centric (COCOMO II) for effective use in a generic case with no personnel or schedule specified. \fixme{Does this need more explanation or concrete examples?}

\section{Evaluating the Effectiveness of a Visualization}

We decided to examine whether the reusable visualization tool is advantageous to the effeciveness of the visualization, i.e., if visualizations built with the tool are likely to be more effective in conveying the information than visualizations built without the tool. To some degree, this can be done with several different methods. Some methods, such as the methods of \citet{tufte_visual_1986} or \citet{zuk_heuristics_2006}, are suitable for assessing data visualizations in general, while others, such as the methods of \citet{kraak_cartographic_1998} or \citet{schlichtmann_visualization_2002}, are specific to geographical visualizations. It is notable that none of the methods provides formal, computable means for evaluation. According to \citet{kraak_cartographic_1998}, \emph{evaluating geographical visualizations is predominantly done by estimating the visualization subjectively in relation to its context}. This makes objective effectiveness evaluation complicated.

The principles by \citet{tufte_visual_1986}, such as data-ink ratio presented in section \ref{section:visualizationprinciples} can be used to evaluate the visualization. Another method for evaluation is presented by \citet{azzam_j-b_2013}. In this method, data representation truthfulness if emphasized. In other words, the visualization is evaluated based on how truthfully the visualization represents the data. However, the correctness of these methods is disputed \citep{kosslyn_graphics_1985,inbar_minimalism_2007}. Moreover, the methods are too abstract for effective evaluation.

Visualization heuristics by \citet{zuk_heuristics_2006} presented in section \ref{section:visualizationprinciples} are significantly more concrete than the methods presented in the previous paragraph and can be used for analyzing a visualization. \emph{Therefore, we decided to use the heuristics for evaluating the tool's effect on visualization effectiveness.}

Geovisualization-specific evaluation methods consist of the methods of \citet{kraak_cartographic_1998} and \citet{schlichtmann_visualization_2002}. The former concentrates on the phrase ``\emph{how} do I say \emph{what} \emph{to whom} and \emph{is it effective}''. The phrase encourages the evaluator to evaluate the selected method and its relation to the data and the target audience. While the method is useful for designing the visualization, it is too abstract for assessing visualizations effectively. The latter consists of a number of mapping objectives presented in section \ref{subsection:effectivemaps}. When compared to the former, it is considerably more concrete, enabling more effective formal evaluation. \emph{Therefore, we decided to complement the heuristics presented in the previous paragraph with the mapping objectives by \citet{schlichtmann_visualization_2002}}.

Due to the fact that the sister project method selected for efficiency evaluation aims to produce as similar results as possible, we deemed it unpractical to use the sister projects for evaluating the visualization effectiveness. \emph{Therefore, we decided to evaluate the tool qualitatively by examining whether it benefits the visualizer in terms of conforming to the heuristics and achieving the objectives.} This conforms to the statement of \citet{kraak_cartographic_1998} related to subjective evaluation of geovisualization.
