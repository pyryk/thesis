%!TEX root = thesis.tex

\chapter{Approach and Methods}
\label{chapter:methods}

In this chapter, we define the research approach used for finding the answers to our research questions. Additionally, we present several methods for evaluating the case properties needed for the research approach, and of those, we pick the most suitable.

\section{Research Approach}

In order to find answers to the research questions\footnote{How does a reusable software system affect the \emph{efficiency} of building geographical visualizations?}\footnote{How does a reusable software system affect the \emph{effectiveness} of geographical visualizations?}, we decided to conduct a \emph{constructive study}. Constructive research excels at finding answers to questions of type ``how useful is system X'' \citep{jarvinen_tutkimustyon_2012}, making it the most suitable research type for this thesis.

In practice, conducting a constructive study involves designing an artifact and evaluating its effect \citep{jarvinen_tutkimustyon_2012}. Therefore, we decided to build a reusable geographical visualization tool, evaluating its effect on the effectiveness of the visualization and the efficiency of building the visualization. This can be done by performing a \emph{case study}, i.e. observing one or more visualization cases and evaluating the relevant properties (in this work, effectiveness and efficiency) in those cases.

\section{Evaluating Software Reuse Effectiveness}

In section \ref{section:successfullreusefactors}, we concluded that it is critical to analyze and measure software reuse. Several methods for analyzing software reuse exist. \citet{frakes_software_1996} present six general types of software reuse analysis models: cost-benefit analysis models, maturity assessment models, amount-of-reuse models, failure modes analysis, reusability assessment models and reuse library metrics. 

Cost-benefit analysis models consider both development costs for the reusable component and the reuse productivity and quality benefits. Maturity assessment models categorize the matureness of a systematic software reuse program. Amount of reuse metrics assess the proportion of reused software in the software system created. Software reuse failure modes model introduces a set of reuse failures which are then used to assess a systematic reuse program. Reusability assessment models aim to analyze various software attributes to assess the reusability of a piece of code. Reuse library metrics concentrate on the reusability of software component libraries instead of single components.

Not all the models discussed above are applicable to this type of research. For instance, as maturity assessment models assess a reuse program as a whole instead of single reusable piece of software, it can not be used for this type of work. Moreover, it should be noted that these models are high-level analysis tools which do not take a stance on how to measure the detailed data required by the measurements.

In their review of multiple case studies, \citet{mohagheghi_quality_2007} list several methods as alternatives for analyzing software reuse. Of them, notable ones include \emph{controlled experiments}, \emph{case studies}, \emph{surveys} and \emph{experience reports}. However, as the scope of this work does not allow for large sample sizes required by controlled experiment method, it is not a feasible alternative for this study. Moreover, using experience reports requires a considerable experience on creating and using reusable software.

According to \citet{kitchenham_evaluating_1998}, one method for conducting a case study is using a \emph{sister project} comparison. In sister project comparison, a minimum of two different, but sufficiently similar software projects observed. In at least one of the projects, a new method is employed, while in at least one project, the old method is in use. All other practices and aspects should be left unchanged. \citet{mohagheghi_quality_2007} argue that this kind of comparison is applicable for analyzing software reuse effectiveness for a specific piece of software. The sister project case study is appropriate when no systematic reuse program exists or when there are other barriers impeding the use of models presented by \citet{frakes_software_1996}. Moreover, it is possible to create the sister project synthetically by building the same kind of application twice, once with and once without the reusable component.

As presented above, in the higher abstraction level, analyzing projects is fairly straightforward. However, the actual low-level measurements for reusing software are much more complex. \citet{mohagheghi_quality_2007} argue that measuring software reuse effectiveness precisely is difficult due to a number of factors: 1) Metrics are difficult to validate since there is no universally accepted definition of ``quality'' in software products, and 2) the productivity of development is difficult to measure and therefore highly subjective, vague or even erroneous metrics are utilized.

Both \citet{frakes_software_1996} and \citet{mohagheghi_quality_2007} agree that the size of the code needed to be written correlates inversely to the development productivity. Moreover, research by \citet{banker_software_1993,gill_cyclomatic_1991} show that software code complexity correlates inversely to the software maintenance productivity. Due to the nature of software maintenance and the fact that it is often hard to distinguish small-scale software development and maintenance \citep{chapin_types_2001}, it is likely that this principle applies to developing new software to some degree as well. Therefore, it seems evident that in order to enable productive use of reusable software, the new code to be written (outside the reusable components) should be made simple and concise.

The conciseness of the code can be measured by several different metrics. According to \citet{fenton_software_1998}, the number of lines in program source code is only one perspective. It can be complemented by measuring the functionality and complexity of the program.

According to \citet{fenton_software_1998}, the most commonly used metric for program size is its length, i.e., the number of lines of source code. The metric can be refined by only considering effective lines, ignoring lines consisting of comments and whitespace. However, \citet{fenton_software_1998} encourages the use of both effective and physical line counts to determine the size of a program.

The functionality of the program can be determined with several different metrics. \citet{fenton_software_1998} presents the function point approach, which uses the number and complexity of external inputs and outputs, object point approach which uses the number of different screens and reports involved in the application, and so-called ``bang metrics'' which use the total number of primitives in the data-flow diagram of the program. It should be noted that all of these methods are highly subjective and only provide speculative metrics.

Also the complexity of the program can be determined with several different techniques. According to \citet{fenton_software_1998}, the complexity of the program (solution) should ideally be not higher than the problem complexity. According to them, in the ideal case it is possible to determine the complexity of the program by determining the complexity of the problem. However, this is not usually the case in real-life applications. \citet{mccabe_complexity_1976} presents a computational approach for determining the complexity of an application implementation. His approach involves counting the number of cycles in the program flow graph. The advantage of this approach when compared to the method presented by \citet{fenton_software_1998} is that it can be used to compute complexity differences of multiple implementations of the same problem.

In addition to the metrics presented above, \citet{halstead_elements_1977} presents a number of complexity measures. These measures can be used to estimate software size, difficulty level and the needed effort solely based on the code. The methods involve determining the number of operators (e.g., function calls) and operands (e.g., variables) in the program source code. These properties are then used to to approximate the higher-level software properties. Unlike the metrics of \citeauthor{mccabe_complexity_1976} or \citeauthor{fenton_software_1998}, \citeauthor{halstead_elements_1977} metrics include an explicit measure for development effort. Therefore, the metrics facilitate the approximation process considerably.

Another approach for estimating program size is using the COnstructive COst MOdel (COCOMO) \citep{boehm_software_1981}. While COCOMO is most typically used for beforehand software project cost estimation, it can also be used to estimate effort. However, COCOMO and its variants are either too vague (COCOMO 81) or too process-centric (COCOMO II) for effective use in a generic case with no personnel or schedule specified. \fixme{Does this need more explanation or concrete examples?}

\section{Evaluating the Effectiveness of a Visualization}

We decided to examine whether the reusable visualization tool is advantageous to the effeciveness of the visualization, i.e., if visualizations built with the tool are likely to be more effective in conveying the information than visualizations built without the tool. This can be done with several different methods. 

The principles by \citet{tufte_visual_1986}, such as data-ink ratio presented in section \ref{section:visualizationprinciples} can be used to evaluate the visualization. Another method for evaluation is presented by \citet{azzam_j-b_2013}. In this method, data representation truthfulness if emphasized. In other words, the visualization is evaluated based on how truthfully the visualization represents the data. Third method for evaluation is presented by \citet{kraak_cartographic_1998}, concentrating on the phrase ``\emph{how} do I say \emph{what} \emph{to whom} and \emph{is it effective}''. The phrase encourages the evaluator to evaluate the selected method and its relation to the data and the target audience.

Visualization heuristics by \citet{zuk_heuristics_2006} presented in section \ref{section:visualizationprinciples} and objectives by \citet{schlichtmann_visualization_2002} presented in section \ref{subsection:effectivemaps} can also be used to evaluate the visualization. When compared to methods mentioned in the previous paragraph, there are arguably more concrete and thus enable easier evaluation.

It is notable that none of the methods presented above provide concrete, computable means for determining the effectiveness. Indeed, \citet{kraak_cartographic_1998} states that evaluating map visualization effectiveness is predominantly done by estimating the visualization subjectively in relation to its context. However, the methods presented above can still be used as a basis when examining the visualizations.

\section{Research Methods Chosen for the Analysis}

We attempt finding answers for the research questions by designing a reusable geographical visualization tool and evaluating its effect using a case study with several visualization cases. We decided to concentrate on the constructing the visualization (i.e., steps 4-5 of \citet{schlichtmann_visualization_2002} and step 4 of \citet{slocum_thematic_2014}), excluding the aspects related to determining the objectives of the visualization and obtaining the data. The reason for this was that by nature, a software utility primarily benefits the construction phase.

For examining the success of the reusable component, we considered both the methods presented by \citet{frakes_software_1996} and \citet{mohagheghi_quality_2007}. As stated in the previous section, several of the methods are only applicable for assessing large-scale reuse programs or cases with large sample size and therefore were not considered for this work. Of the remaining methods, a cost-benefit analysis was selected. For studying costs and benefits of reuse, the sister project method was deemed most applicable. 

In order to measure the effort needed as reliably as possible, we decided to use a diverse sets of different metrics. Software size and complexity metrics by \citet{fenton_software_1998} were used, with the exception that measuring software complexity is done with the method by \citet{mccabe_complexity_1976} as it allows comparing different implementations of similar applications. The metrics were complemented with Halstead measurements for difficulty and effort.

For examining the visualization effectiveness, all the methods presented above could be used in combination. However, to keep the scope of this work manageable, we decided to concentrate on the concrete data visualization heuristics by \citet{zuk_heuristics_2006}, complementing the evaluation with geovisualization-specific perspective by using thematic mapping objectives by \citet{schlichtmann_visualization_2002}. The visualization effectiveness cannot be evaluated by comparing the implementation to sister projects, because the resulting visualizations are planned to be as similar as possible. Therefore, we decided to evaluate the implementation qualitatively by examining whether the implementation benefits the visualizer in terms of conforming to the heuristics and achieving the objectives.

For gathering data, we implemented several typical map visualizations with and without the framework. The types of visualizations were selected to obtain data about a wide variety of different map visualizations while still concentrating to the most frequently used methods.

