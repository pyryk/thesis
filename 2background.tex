%!TEX root = thesis.tex

\chapter{Software Reuse}
\label{chapter:reuse}

In this chapter, I conduct a brief study of software reuse. In the study, I concentrate on different reuse techniques and their characteristics, advantages and disadvantages. I also study the success factors of software reuse and methods for analyzing and evaluating software reuse.

\citet{krueger_software_1992} presents software reuse as a process of reusing existing software code (applications, libraries, functions or single lines) when building new software. On the other hand, according to \citet{mohagheghi_empirical_2008}, reuse is not restricted to code, but can also refer to other software assets such as design. However, both agree that software reuse combines several different existing pieces of code (and possibly other assets) along with new assets which are specific for the application in question. According to \citet{mcilroy_mass-produced_1969} and \citet{boehm_managing_1999}, it is one of the most effective techniques of reducing the development time and cost of complex software products.

\section{Software Reuse Advantages \& Disadvantages}

When used appropriately, software reuse has several benefits. In their overview of multiple case studies, \citet{mohagheghi_empirical_2008} discovered that in most cases, using reused software components resulted in a considerably lower number of software defects and better productivity. Several of the studies implied that reusing software is also beneficial for software complexity and product time-to-market. However, it should be noted that since the overview only addresses case studies, its results should not be considered universally applicable.

The majority of software reuse studies indicate that software reuse decreases the effort needed (e.g., \citealt{mcilroy_mass-produced_1969, boehm_managing_1999, mohagheghi_empirical_2008}). However, concrete evidence for this is difficult to find \citep{mohagheghi_empirical_2008}.

Given its lucrative advantages, software reuse is definitely beneficial for many software systems. However, according to \citet{krueger_software_1992}, software reuse can be problematic and even disadvantageous. Learning to use a specific piece of reusable software often takes considerable effort. Moreover, finding suitable code fragments may also prove to be a challenge. For uncomplicated software systems and especially reusable components, it may not be worth the effort. Therefore, developer needs to carefully consider all sides of reusing when building a software system. According to \citet[chap.~1.3]{krueger_software_1992}, for successful software reuse scenario, the amount of intellectual effort between the concept and implementation of the system must be as low as possible. In practice, this means that the value of the reused component must be as high as possible for the developed system, while the implementation cost (resources needed to take the reusable component into use) should be relatively low. 

\section{Factors for Successful Software Reuse}
\label{section:successfullreusefactors}
\citet{frakes_success_1994} present six critical factors for successful software reuse: management, measurement, legal, economics, design for reuse, and libraries. Some of the factors are relevant only for a corporate-level reuse program, but many are critical for smaller scale reuse as well.

Successful reuse requires the \textbf{management} to commit to a long-term, top-down support, because reusing software may require years to pay off the costs. Also, \textbf{measurement} of reuse is vital to reuse software successfully. Both \emph{reuse level} (the ratio of reused software to total software) and \emph{reuse factors} (things affecting the increase of reuse) should be measured.

\textbf{Legal} issues are also important to consider when reusing software. Specifically, the stakeholders should agree on the rights and responsibilities of providers and consumers of reusable software. Moreover, using software with conflicting licenses may cause problems.

\textbf{Economics} present a challenge in systematic reuse scenarios. Measuring reuse costs is not straightforward. Frequently, the costs of creating a reusable component are compensated by benefits in some other project using the component.

\textbf{Designing for reuse} requires a degree of domain knowledge. This necessitates the study of the domain when creating \emph{domain-specific} reusable software. In addition to that, reusable software design requires effort on encapsulation, abstraction and interfaces.

Lastly, reusable software \textbf{libraries} are required to fully benefit from the reusability effort. Libraries enable storing, retrieving and finding the reusable software.

\section{Analyzing Software Reuse}
According to \citet{krueger_software_1992}, in order to analyze software reuse techniques, the specific technique should be analyzed using four \emph{dimensions}: \emph{abstraction, selection, specialization and integration}. I present the dimensions below.

\paragraph{Abstraction} is the process of making a piece of software more generic, thus making it applicable to a wider range of software projects. Software reuse is almost always based on abstraction, but according to \citet{krueger_software_1992}, raising the abstraction level has proven to be difficul. Therefore, building reusable software is a challenging process.

\paragraph{Selection} facilitates finding, comparing and choosing suitable pieces of software. For example, libraries or frameworks aid selection by bundling and structuring the software components.

\paragraph{Specialization} is the process of making the abstracted component more specific. Usually, reusable pieces of software achieve specialization by parameterizing the software or making it transformable.

\paragraph{Integration} facilitates providing the software with reusable components, for example with a mechanism to import relevant modules or functions to the software.

\section{Software Reuse Methods}

Software reuse is not a single, uniform procedure or technique. Several different reuse techniques exist to cater different needs. Consequently, different reuse methods excel at different areas. In order to describe the advantages and disadvantages of the methods, I describe them using the reuse dimensions presented in the previous section.

\citet{krueger_software_1992} and \citet{sametinger_software_1997}, among others, present and analyze software reuse methods. From these, I have selected the most relevant for web environment, presenting those below.

\subsection{High-Level Languages}
High-level languages denote programming languages which are designed to be on a high abstraction level. In practice, this means that such languages contain features which are not necessary for a programming language but benefit or speed up the development. Traditional examples of these kind of features are automatic memory allocation \citep{krueger_software_1992} and language constructs such as exceptions \citep{mitchell_concepts_2003}. More modern high-level language features are value type checking systems and abstracted support for parallel operations using futures \citep{totoo_haskell_2012}. 

It should be noted that the high-levelness of a language is a \emph{relative} property. Therefore, it is not possible to determine the requirements for a high-level language per se, only high-levelness of languages compared to other languages. For example, \citet{krueger_software_1992} considers all programming languages above the abstraction level of an assembly language\footnote{A low-level language with one-to-one translation to machine code instructions for a computer architecture \citep{salomon_assemblers_1993}} high-level languages. On the other hand, \citet{carro_high-level_2006} consider, e.g., lack of automatic memory management or type system a sign of lower-level language.

High-level languages \emph{abstract} frequently used procedures into seemingly uncomplicated operations, thus reducing the work and cognitive capacity needed for developing the application. \citep[chap.~3]{krueger_software_1992}

The number of elementary high-level language constructs is usually relatively low. Therefore, it is possible for programmers to master the use of those constructs with sufficiently little effort. In practice, this renders \emph{selection} of the constructs unproblematic. \citep[chap.~3]{krueger_software_1992}

The developer can achieve the \emph{specialization} of high-level language features by parameterizing the constructs, either implicitly or explicitly. For example, when instantiating a class in Java, the only parameterization needed for memory management is the actual object instance. However, e.g., exception handling always requires at least the logic needed for handling the exception. \citep[chap.~3]{krueger_software_1992}

\emph{Integration} of high-level language features is automatically done when compiling the software code. However, due to the nature of high-level languages, it is usually not possible to mix-and-match different programming languages easily in the same program. \citep[chap.~3]{krueger_software_1992} % FIXME maybe passive => active

The advantages of high-level languages consist mainly of the decreased need for manually developing frequently needed procedures. In practice, high-level languages provide a seemingly simple interface to complex functionality. This effectively makes them reusable software components. In practice, using high-level languages can yield a productivity gain up to 500 \%. \citep[chap.~3]{krueger_software_1992}

The main disadvantage of using high-level languages is the potential decrease in performance. As with any software reuse, high-level programming languages abstract the supported procedures by making them more generic. This often leads to additional complexity and unnecessary operations on the compiled program. However, several compile-time optimization procedures exist for decreasing the effect. \citep{carro_high-level_2006}

On the web, the technologies used on the client-side are inherently fixed to descendants of HyperText Markup Language (HTML)\footnote{\url{http://www.w3.org/TR/html5/}}, Cascading Style Sheets (CSS)\footnote{\url{http://www.w3.org/Style/CSS/}} and ECMAScript\footnote{\url{http://www.ecma-international.org/ecma-262/5.1/}}. Therefore, web application languages are relatively high-level by definition. However, web developers can still raise the abstraction level by using, e.g., CoffeeScript \citep{ashkenas_coffeescript_2009} instead of JavaScript or LESS \citep{sellier_less_2009} instead of CSS.

\subsection{Design and Code Scavenging}

Design and Code scavenging refers to the technique of scavenging pieces of software \emph{ad hoc} from existing software systems for use in a new software system  \citep[chap.~4]{krueger_software_1992}. The aim of this technique is to reduce the amount of work needed to build the system. For example, when building an User Interface (UI) component for choosing a date, the developer may scavenge the code for a calendar from an older software system.

Scavenging can be performed by using one of the two different approaches. The developer can perform the scavenging without modifications to the code in the target code base (code scavenging). Alternatively, scavenging can be performed by modifying the details of the scavenged code (design scavenging) \citep[chap.~4]{krueger_software_1992}. The \emph{abstraction} gained by scavenging is therefore mostly informal and in some cases even its existence is questionable \citep[chap.~3]{sametinger_software_1997}. Usually, there is no ``hidden part'' of the abstraction but the developer must maintain the functionality of all the code himself \citep[chap.~3]{sametinger_software_1997}.

Usually there is no formal mechanism or support for \emph{selecting} pieces of software to be scavenged. Therefore, the developer must rely on his memory, experience and word-of-mouth in order to find suitable pieces of software. \citep[chap.~3]{sametinger_software_1997}

The developer \emph{specializes} the scavenged assets by manually editing the scavenged source code. While it is often the fastest method of acquiring results, this requires the developer to deeply understand the scavenged implementation. It can also lead to fragmentation and maintainability issues in the future. \citep[chap.~4]{krueger_software_1992}

Typically, the developer \emph{integrates} the scavenged code by copying and pasting the code to the target source code file. This may lead to namespace collisions between original and scavenged code. This, in turn, may result in the need for refactoring the code. \citep[chap.~4]{krueger_software_1992}

The main advantages of design and code scavenging are the ability to quickly include existing functionality to new software systems \citep[chap.~4]{krueger_software_1992}. Moreover, it is usually not needed to prepare the code to be scavenged before scavenging it. Therefore, practically all available code is reusable by scavenging. Consequently, the number of scavengable pieces of software is often significantly larger than when using any other reuse method. 

However, finding suitable pieces of software for scavenging is hard. Moreover, scavenging pieces of software often does not decrease the \emph{cognitive distance} between the target and implementation of the system. It may also create issues with maintainability of the software. \citep[chap.~4]{krueger_software_1992}

\subsection{Source Code Components}

Using source code components is a method of reuse which enables choosing and using software components from a component repository \citep[chap.~3]{sametinger_software_1997}. Software component can be any piece of code, but in practice, components usually consist of one or more functions, modules or classes \citep[chap.~3]{sametinger_software_1997}. An example of a source code component is a trigonometry module which contains functions for sine, cosine and tangent calculations. When the developer needs to calculate sines in her program, she searches a component repository for trigonometry components and utilizes the component found in her own program \citep[chap.~5]{krueger_software_1992}.

Ideally, source code components \emph{abstract} the implementation details of the component inside. This means that the required cognitive distance between the concept and the implementation of the software system is lower when using source code components instead of, e.g., code scavenging. 

In order to use the component, the developer must be able to find it and to know what it does \citep[chap.~5]{krueger_software_1992}. Therefore, in order to be \emph{selectable}, source code components should be accompanied by abstract names (function names) and descriptions of the functionality provided \citep[chap.~5]{krueger_software_1992}. The names should describe \emph{what} the component does instead of \emph{how} it does it \citep[chap.~5]{krueger_software_1992}. The developer can then use these names for reasoning about the purpose of the component and finding the component in source code component repositories.

The developer can \emph{specialize} the source code component by modifying the source code \citet[chap.~5]{krueger_software_1992}. However, this technique yields unwanted consequences explained in the previous section. Therefore, many components support specialization by parameterization. For example, the programmer could provide the sine function the angle in question. Additionally, when integrating the trigonometry module to her software system, the programmer could specify if the functions should use degrees or radians. In some cases, the developer can specialize the components via subclassing \citep[chap.~5]{krueger_software_1992}.

All modern programming languages support \emph{integration} of reusable source code components written in the same language. Usually, the procedure is a very simple addition of source code files, which requires little to no effort on the programmer side. However, all source code components can't be used in the same program due to conflicts, e.g., in naming and value types \citep[chap.~5]{krueger_software_1992}.

The main advantages of using source code components are the abstraction provided and the organized nature of the component repositories. Ideally, the repositories provide a search functionality so that even developers with no previous experience on the component domain can find the components needed. Moreover, the abstraction level and the hiding of implementation details decreases the cognitive distance between the concept and the implementation of the system. They also reduce the size of source code needed to be written.

The main disadvantages of the source code components lie in the fact that the functionality must be deliberately designed to support reuse. The abstraction of the components is a major challenge \citep[chap.~5]{krueger_software_1992} in designing source code components. Additionally, the component repositories need administration and maintenance.

\subsection{Application Generators}

Application generators are usually domain-specific generators which take very high level instructions (specifications) as input and then output significantly lower level software code (implementation) \citep[chap.~7]{cleaveland_building_1988,krueger_software_1992}. On fundamental level, application generators differ from high-level language compilers mainly by being designed to work on a narrow domain. Consequently, appplication generators are generally able to support considerably higher-level instructions \citep[chap.~7]{krueger_software_1992}. Unlike source code components, the reused components generated by application generators are typically not encapsulated or separated \citep[chap.~3]{sametinger_software_1997}.

Application generators \emph{abstract} the concept or specification of the software system, hiding the actual implementation completely from the user of the generator \citep{cleaveland_building_1988}. However, in some cases the developer may need to modify the output of the generator which essentially removes the abstraction.

The abstraction level of application generators is usually very high, rendering reasoning about the purpose of the generator fairly easy \citep[chap.~7]{krueger_software_1992}. Therefore, in principle, \emph{selecting} application generators is moderately easy. However, since application generators are usually suited for a very narrow domain, it may be difficult to find a suitable generator \citep[chap.~7]{krueger_software_1992}.

Typically, software systems generated with application generators consist of variant and invariant parts \citep[chap.~7]{krueger_software_1992}. Invariant part is the part of the program which the developer using the generator can't modify. The developer \emph{specializes} the program by modifying the variant part. There are several methods of modifying the variant part. One of the simplest may be straightforward parameterization: the developer chooses the parameters of the system from a predefined set of alternatives. This method makes using the generation extraordinarily easy. However, it also limits the resulting application considerably.

On the other end of the spectrum, the application generator may require the variant parts to be inputted using a domain-specific or a general-purpose programming language. This makes the application generator incredibly versatile, but requires both more domain-specific and programming knowledge.

Typically, application generators generate complete applications which do not require further \emph{integration} \citep[chap.~7]{krueger_software_1992}. However, occasionally, the resulting applications are not independent per se, but require integration to other systems. This may be an issue since often it is not possible to select the integration interfaces freely, but to use the ones provided by the generator.

One of the main advantages of the application generators is the abstraction they provide. In some cases, the application generators may even require no programming language knowledge as long as the user has relevant domain-specific knowledge \citep{horowitz_survey_1985}. Moreover, application generators excel when there is a need for building multiple similar applications \citep[chap.~7]{krueger_software_1992}. 

However, application generators require an unambiguous mapping between the specifications and implementation details \citep[chap.~7]{krueger_software_1992}. Moreover, building application generators requires a reliable, generic implementation and user interfaces for developers \citep{cleaveland_building_1988}. Therefore, building application generators requires comprehensive domain-specific knowledge in addition to extensive software development expertise.

\subsection{Software Frameworks}

Software frameworks are a reuse technique which combines the use of software components and programming patterns \citep{johnson_frameworkscomponents+_1997}. Therefore, \citet{johnson_frameworkscomponents+_1997} argues that software frameworks enable creating reusable software design. Unlike software components, frameworks are designed to be extended by providing case-specific functionality \citep{lambeau_software_2011}. Another definition of software frameworks is that they are a collection of consolidated components, i.e., components which share the design, interfaces, and, to some degree, implementations \citep{johnson_frameworkscomponents+_1997}. It should also be noted that software frameworks are typically strictly object-oriented reuse technique \citep{johnson_frameworkscomponents+_1997}.

Largely, software frameworks \emph{abstract} the implementation details the same way that components do, i.e., providing a higher-level interface for the low-level operations. In addition to that, frameworks abstract software \emph{design patterns} used to provide a more complete architecture and functionality.

Typically, the number of applicable frameworks is considerably smaller than, e.g., the number of applicable source code components \citep{fayad_enterprise_2000}. Therefore, the \emph{selection} of frameworks may seem relatively straightforward. However, as frameworks are by definition more complex than single software components \citep{johnson_frameworkscomponents+_1997}, selecting the right framework of a purpose may be considerably more difficult \citep{fayad_enterprise_2000}.

\emph{Specializing} frameworks is greatly dependent on the purpose and design of the framework. As some frameworks are designed as domain-specific \citep{johnson_frameworkscomponents+_1997}, it is typically not needed to specialize the system extensively. According to \citet{brugali_framework_1997}, frameworks can usually be specialized in an object-oriented fashion: using parameters and subclasses to fine-tune functionality \citep{brugali_framework_1997}.

Typically, software frameworks are \emph{integrated} to other frameworks and to larger software systems. However, the majority of frameworks are designed for adaptation instead of integration \citep{mattsson_framework_1999}. This leads to integration problems. \citet{mattsson_framework_1999} describe several framework integration problems, e.g., architecture, design and pattern mismatches. Nonetheless, the developer can overcome most of the problems by using a number of solutions, such as separating the concerns clearly and wrapping the functionality to compliant components \citep{mattsson_framework_1999}.

One of the main advantages of frameworks is that they enable a complete, potentially opinionated approach for reusing software while preserving the possibility for customization \citep{johnson_frameworkscomponents+_1997}. The main disadvantages of using software frameworks consist of occasional steep learning curve and challenges in integration \citep{fayad_object-oriented_1997}.

\section{Evaluating Software Reuse}
\label{section:evaluatingreuse}

In section \ref{section:successfullreusefactors}, I concluded that it is critical to analyze and measure software reuse. \citet{frakes_software_1996} divides reuse evaluation methods to high-level and low-level methods. High-level methods emphasize the development process and higher-level properties of software, while low-level methods consist of the methods for measuring individual software projects. In addition, I present several metrics for measuring software code properties needed by the higher-level methods.

\subsection{High-level Evaluation}

Several methods for high-level software reuse analysis exist. \citet{frakes_software_1996} present six methods for high-level analysis: \emph{cost-benefit analysis}, \emph{maturity assessment} models, \emph{amount-of-reuse} models, \emph{failure modes analysis}, \emph{reusability assessment} models and \emph{reuse library metrics}. Cost-benefit analysis models consider both development costs for the reusable component and the reuse productivity and quality benefits. Maturity assessment models categorize the matureness of a systematic software reuse program. Amount of reuse metrics assess the proportion of reused software in the software system created. Software reuse failure modes model introduces a set of reuse failures which are then used to assess a systematic reuse program. Reusability assessment models aim to analyze various software attributes to assess the reusability of a piece of code. Reuse library metrics concentrate on the reusability of software component libraries instead of single components.

Different methods are suitable for different use cases. For instance, maturity assessment models assess a reuse program as a whole instead of single reusable piece of software. On the other hand, cost-benefit models can also be used for assessing single piece of software.

\subsection{Low-level Evaluation}

The models presented above are high-level analysis tools which do not take a stance on how to measure the detailed data required by the measurements. Hence, the models need to be complemented with lower-level, more detail-oriented methods. In their review of multiple studies, \citet{mohagheghi_quality_2007} list several methods for software reuse analysis. Of them, notable ones include \emph{controlled experiments}, \emph{case studies}, \emph{surveys} and \emph{experience reports}. In controlled experiments, a large set of projects, some of which employ the evaluated method, is analyzed. Case studies concentrate on fewer studies but often involve a more thorough analysis of each case. Surveys and experience reports involve qualitative analysis of the cases. 

One method for performing a case study is the sister project comparison. According to \citet{kitchenham_evaluating_1998}, in sister project comparison, a minimum of two different, but sufficiently similar software projects are observed. In at least one of the projects, a new method is employed, while in at least one project, the old method is in use. All other practices and aspects should be left unchanged. \citet{mohagheghi_quality_2007} argue that this kind of comparison is applicable for analyzing software reuse effectiveness for a specific piece of software. The sister project case study is appropriate when no systematic reuse program exists or when there are other barriers impeding the use of other models presented by \citet{mohagheghi_quality_2007}. Moreover, it is possible to create the sister project synthetically by building the same kind of application twice, once with and once without the evaluated method.

\subsection{Software Code Metrics}

As presented above, in the higher abstraction level, analyzing projects is fairly straightforward. However, the actual low-level measurements for reusing software are much more complex. \citet{mohagheghi_quality_2007} argue that measuring software reuse effectiveness precisely is difficult due to the following factors: 1) Metrics are difficult to validate since there is no universally accepted definition of ``quality'' in software products, and 2) the productivity of development is difficult to measure. Therefore, according to \citet{mohagheghi_quality_2007}, many projects employ highly subjective, vague or even erroneous metrics.

Both \citet{frakes_software_1996} and \citet{mohagheghi_quality_2007} agree that the size of the code needed to be written correlates inversely to the development productivity. However, according to \citet{fenton_software_1998}, the number of lines in program source code is only one perspective for program size. It should be complemented by measuring the functionality and complexity of the program. Moreover, research by \citet{banker_software_1993,gill_cyclomatic_1991} show that software code complexity correlates inversely to the software maintenance productivity. \emph{Therefore, it seems evident that in order to enable productive use of reusable software, the new code to be written should be made simple and concise.}

The evaluator can measure the conciseness of the code with several different metrics. According to \citet{fenton_software_1998}, the most commonly used metric for program size is its length, i.e., the number of lines of source code. The metric can be refined by only considering effective lines, ignoring lines consisting of comments and whitespace. However, \citet{fenton_software_1998} encourage the use of both effective and physical line counts to determine the size of a program.

The evaluator can determine the functionality of the program with several different metrics. \citet{fenton_software_1998} present the function point, object point and bang metric approaches. \emph{Function point} approach uses the number and complexity of external inputs and outputs. \emph{Object point} approach uses the number of different screens and reports involved in the application. \emph{Bang metrics} use the total number of primitives in the data-flow diagram of the program. According to \citeauthor{fenton_software_1998}, all of these methods are highly subjective and only provide speculative metrics.

Also the complexity of the program can be determined with several different techniques. According to \citet{fenton_software_1998}, the complexity of the program (i.e., solution to a problem) should ideally be not higher than the problem complexity. According to them, in the ideal case it is possible to determine the complexity of the program by determining the complexity of the problem. However, this is not usually the case in real-life applications. \citet{mccabe_complexity_1976} presents a computational approach for determining the complexity of an application implementation. His \emph{cyclomatic complexity} method involves counting the number of cycles in the program flow graph to approximate the program complexity. The advantage of this approach when compared to the method presented by \citet{fenton_software_1998} is that it can be used to compute complexity differences of multiple implementations of the same problem.

In addition to the metrics presented above, \citet{halstead_elements_1977} presents a number of complexity measures. These measures can be used to estimate software size, difficulty level and the needed effort solely based on the code. The methods involve determining the number of operators (e.g., function calls) and operands (e.g., variables) in the program source code. These properties are then used to to approximate the higher-level software properties. Unlike the metrics of \citeauthor{mccabe_complexity_1976} or \citeauthor{fenton_software_1998}, \citeauthor{halstead_elements_1977} metrics include an explicit measure for development effort.

Another approach for estimating program size is using the COnstructive COst MOdel (COCOMO) \citep{boehm_software_1981}. While COCOMO is most typically used for beforehand software project cost estimation, it can also be used to estimate effort. Several variants of the model exist. All variants provide an approximate measure of a software project effort based on the line counts of software code and, optionally, a number of project-specific factors. However, COCOMO and its variants are either too vague (Basic COCOMO 81) or too process-centric (Intermediate and Detailed COCOMO 81, COCOMO II) for effective use in a generic case with no personnel or schedule specified.

\chapter{Data Visualization}
\label{chapter:dataviz}

In this chapter, I define data visualizations and present several principles for successful data visualization. I also describe processes and methods for visualizing geographical data along with some guidelines for assessing the quality of geovisualizations.

\section{Definition}

According to \citet[chap.~3]{kosara_visualization_2007}, there is no universally accepted definition of visualization. He proposes the following for a ``minimal set of requirements for any visualization'':

\begin{itemize}
	\item It is based on (non-visual) data
	\item It produces an image
	\item The results are readable and recognizable
\end{itemize}

According to him, visualizations can also have other properties or qualities, such as interaction or visual efficiency. However, the requirements above are the ones needed for technical definition of the term. Moreover, it should be emphasized that according to this definition, visualization is the \emph{process} itself, not the result of it.

\citet[chap.~4]{kosara_visualization_2007} separates visualization into two types, \emph{pragmatic} and \emph{artistic} visualization. Pragmatic visualization focuses on the analysis of the data in order to show its relevant characteristics as efficiently as possible. Artistic visualization on the other hand concentrates on the communication of a concern, not the display of the actual data. Therefore, artistic visualizations may emphasize or even exaggerate some of the features of the data. \citeauthor{kosara_visualization_2007} states that while these types focus on the opposite sides of the visualization spectrum, it may be possible to close the gap using, e.g., interaction.

The first requirement for visualizations by \citet{kosara_visualization_2007} dictates that the visualization is based on data. This is in alignment with the principle of \citet{tufte_visual_1986} which states that visualizations should, above all else, show the data. This is an essential characteristic of \emph{data} visualizations: the visualization is a function which takes data as an input and produces a visual object as an output. In less technical terms, this means that the visualization turns data into visual, effortlessly and efficiently digestible format.

This leads to the fact that the data and visualization are not inherently tied to each other; the visualization ``function'' can be independent of the data. Therefore, it may be possible to create a visualization framework or platform which is able to function on a potentially wide range of data.

The characteristic of turning data into effortlessly digestible format makes visualization extremely important as ``modern society is confronted with a data explosion'' \citep{van_wijk_value_2005}. Not only do we have access to unprecedented amount of data, we also need to increasingly base our actions and thoughts on the data \citep{van_wijk_value_2005}. Without visualization, this approach would not be possible. \emph{Therefore, one of the most important objectives of visualization is to facilitate better understanding of the data.}

\section{Principles for Successful Data Visualization}
\label{section:visualizationprinciples}

The requirements presented in the previous section are sufficient for the definition of data visualization. However, they do not convey any information about visualization quality. In order to discover the characteristics for successful data visualization, additional principles are needed. \citet[p.~13]{tufte_visual_1986} states that excellent graphics (i.e., results of visualizations) consist of ``complex ideas communicated with clarity, precision and efficiency''. In practice, this means that the graphics should emphasize the actual data and its nuances above everything else, while serving a clear purpose. 
% passive, because the actor is truly undefined

In addition to graphics principles presented in the previous paragraph, \citet[p.~93]{tufte_visual_1986} presents the concept of \emph{data-ink}. Data-ink represents the ink used for displaying the data in a visualization. He argues that in an excellent visualization, most, if not all, ink used should contribute to display of the data. However, research by \citet{inbar_minimalism_2007} suggests that maximizing the share of data-ink may not be beneficial to the user experience of the visualization. E.g., axis lines in a chart are not data-ink according to the definition of \citeauthor{tufte_visual_1986}. However, they may be beneficial to the user experience of the chart by providing visual structure.

The principles presented above are essential, but too abstract in order to be used as a sole basis for defining a good visualization. However, when combined with \citeauthor{kosara_visualization_2007}'s data visualization definition stated above, the principles become considerably more useful and concrete. \citet{azzam_j-b_2013} propose an adapted version of the definition by \citet{kosara_visualization_2007}. According to them, ``Data visualization is a process that (a) is based on qualitative or quantitative data and (b) results in an image that is representative of raw data, which is (c) readable by viewers and supports exploration, examination and communication of the data''. The most significant differences are that the definition complements the second requirement of \citeauthor{kosara_visualization_2007} (``It produces an image'') by requiring the produced image to represent the data truthfully. It also requires the visualization to be enlightening instead of just readable. This definition effectively combines the definition by \citet{kosara_visualization_2007} with the principle of showing data introduced by \citet{tufte_visual_1986}. The adapted definition facilitates the process of creating a successful data visualization by offering a more concrete version of Tufte's principles. It gives the developer of the visualization a slightly more concrete checklist for representing the data: make sure the representation does not (a) omit or (b) overrepresent any information, and (c) helps the viewer gain knowledge \citep{azzam_j-b_2013}. 
% passive, because emphasizing the principles instead of their "users"


\begin{table}
\begin{tabular}{|p{4.2cm}|p{8.8cm}|}
\hline
\textbf{Identifier} & \textbf{Description} \\ 
\hline
Visual variable & Ensure visual variable has sufficient length \\
Color order & Don't expect a reading order from color \\
Color size & Color perception varies with size of colored item \\
Local contrast & Local contrast affects color \\
Color blindness & Consider people with color blindness \\
Preattentive benefits & Preattentive benefits increase with field of view \\
Size variation & Quantitative assessment requires position or size variation \\
Graphic dimensionality & Preserve data to graphic dimensionality \\
Most data & Put the most data in the least space \\
No extra ink & Remove the extraneous (ink) \\
Gestalt laws & Consider Gestalt Laws \\
Levels of detail & Provide multiple levels of detail \\
Integrate text & Integrate text wherever relevant \\
Overview first & Provide overview first \\
Zoom and filter & Zoom and filter out uninteresting data \\
Details on demand & Provide details on demand \\
Relate &  Consider relationships among items \\
Extract & Allow extraction of data and its subsets \\
History & Keep history of actions \\
Uncertainty & Expose uncertainty \\
Relationships & Concretize relationships \\
Domain Parameters & Determination of domain parameters \\
Multivariate & Provide multivariate explanation \\
Cause \& effect & Formulate cause \& effect \\
Hypotheses & Confirm Hypotheses \\
\hline
\end{tabular}
\caption{Heuristics presented by \citet{zuk_heuristics_2006} with generated identifiers.}
\label{table:heuristics}
\end{table}

For the most concrete principles, \citet{zuk_heuristics_2006} provide a list of heuristics for visualizations established in perceptual, cognitive and usability research. The list combines several heuristics from multiple heuristics sets by \citet{shneiderman_eyes_1996,zuk_theoretical_2006,amar_knowledge_2004}. While the combination results in potentially conflicting or redundant heuristics, it nevertheless provides a concrete checklist for making successful visualizations. I present the heuristics, along with generated identifiers for easier referring, in table \ref{table:heuristics}.

\section{Visualizing Geographical Data}

As geographic data is associated with a specific location, the most natural way of visualizing it is by using a map \citep[chap.~1]{kraak_cartographic_1998,kraak_cartography_2011}. This technique is called \emph{thematic mapping} \citep[chap.~1]{slocum_thematic_2014}. Thematic mapping does not require any specific format of data, except for the geographical dimension \citep[chap.~1]{kraak_cartography_2011}. However, the nature of the data has a great effect on the method, or type, of thematic mapping.

\subsection{Methods for Thematic Mapping}
\label{subsection:mappingmethods}

As I state above, there are several types of geographical data. Many of the types are fundamentally different requiring different visualization methods. Therefore, cartographers have developed several different thematic mapping methods. \citet[chap.~14-18]{slocum_thematic_2014} list some of the most typical ones:

\paragraph{Choropleth Map}

Choropleth maps are used primarily for visualizing data which coincides with predefined enumeration units. This method is the most natural one for situations when the enumeration units are directly linked to data results, such as votes in an election for each voting area. However, choropleth maps are also used to depict ``typical'' values for an area even when in reality, the area is heterogeneous in relation to the measured quality. Typical visualization of choropleth maps is by grouping a specified area using a constant color or a common symbol. Figure \ref{fig:choropleth} depicts an example of a choropleth map. (Ibid.)
% passive, because emphasizing the map type (debatable?)

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=12cm]{images/choropleth-example.png}
    \caption{A choropleth map depicting the composition of the Electoral College in the United States presidential election of 2012 \citep{skidmore_electoral_2012}}
    \label{fig:choropleth}
  \end{center}
\end{figure}

\paragraph{Isarithmic Map}

Isarithmic maps are map visualizations depicting continuous or smooth phenomena. Therefore, isarithmic maps excel at visualizing natural properties such as elevation. The most commonly used type of isarithmic mapping is the contour map. Contour maps consist of the measured property visualized as gradient colors in addition to \emph{contour lines} used as value symbolization. Figure \ref{fig:isarithmic} contains an example of the isarithmic mapping method. (Ibid.)

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=9cm]{images/isarithmic-example.png}
    \caption{An isarithmic map depicting travel times to Helsinki center. \citep{hsl_matka-aikakartta_2014}}
    \label{fig:isarithmic}
  \end{center}
\end{figure}

\paragraph{Dasymetric Map}

Dasymetric mapping is closely related to choropleth mapping, with the exception that in dasymetric mapping, the enumeration units are not predefined, but rather defined by the data coherency. When creating dasymetric maps computationally, the visualizer can approximate the properties by using a number of techniques, as presented in chapter \ref{subsection:supportedvisualizationmethods}. Dasymetric mapping is most naturally used for data with several internally cohesive blocks of area, such as land use (i.e., the distribution of roads, cities, forests etc.). Figure \ref{fig:dasymetric} presents an example of a dasymetric map. (Ibid.)

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=9cm]{images/dasymetric-example.jpg}
    \caption{Dasymetric map depicting land use density in San Francisco \citep{fischer_generalized_2012}.}
    \label{fig:dasymetric}
  \end{center}
\end{figure}

\paragraph{Dot Map}

Dot maps are used to represent data which is associated with locations. With dot maps, the data used can be \emph{true} (truly associated with a single point) or \emph{conceptual} (aggregated to a point). Moreover, the visualizer can cluster or combine data points for less cluttered visualization. Dot maps can be used for visualizing, e.g., store locations or the number of homicides in different cities. Figure \ref{fig:dot} presents an example of a dot map. (Ibid.)

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=8cm]{images/dot-example.jpg}
    \caption{Dot map depicting cholera cases during the London epidemic of 1854 \citep{snow_cholera_1854}.}
    \label{fig:dot}
  \end{center}
\end{figure}

\paragraph{Proportional Symbol Map}

Proportional symbol maps are closely related to dot maps. The typical use case for a proportional symbol map is visualizing ratio variables associated with a location. Unlike dots in dot maps, the symbols on a proportional symbol map are sized proportionally to the data. Symbols can be geometric or pictorial. In addition, their sizes can be determined using several different methods, e.g., purely mathematical scaling or perceptual scaling which takes human visual inaccuracy into account. Figure \ref{fig:proportionalsymbol} presents an example of a proportional symbol map. (Ibid.)
% passive, because emphasizing the symbols instead of the visualizer
% passive, because the actor is (deliberately) undefined; can be visualizer or some other party

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=10cm]{images/proportionalsymbol-example.png}
    \caption{Proportional symbol map depicting the location and magnitude of earthquakes in the Middle East. \citep{globalincidentmap_live_2014}}
    \label{fig:proportionalsymbol}
  \end{center}
\end{figure}

\paragraph{Multivariate Mapping}

Multivariate mapping denotes displaying multiple attributes simultaneously. Visualizers can create multivariate maps in several ways. She can visualize the relevant attributes either with a single map or using a separate map for each attribute. Additionally, she can overlay (place on top of each other) the attributes or combine them (using a single symbol depicting all attributes). Figure \ref{fig:multivariate} presents an example of a multivariate map. (Ibid.)

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=10cm]{images/multivariate-example.jpg}
    \caption{Multivariate map depicting the average temperature and precipitation of Brazil \citep{central_intelligence_agency_temperature_1977}.}
    \label{fig:multivariate}
  \end{center}
\end{figure}

\paragraph{Cartogram}

Cartograms are used to distort the map based on the data. Therefore, visualizers can use cartograms to communicate relative sizes of an attribute in several areas, such as population in each country. This is advantageous when the geographical sizes and attribute values do not correlate, e.g., when some areas with high attribute value are extremely small in size. Figure \ref{fig:cartogram} depicts an example of a cartogram. (Ibid.)

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=10cm]{images/cartogram-example.png}
    \caption{Cartogram depicting the population of the world \citep{hennig_population_2014}.}
    \label{fig:cartogram}
  \end{center}
\end{figure}

\paragraph{Flow Map}

Flow maps are maps with lines or arrows of varying width from one location to another. Therefore, flow maps excel at displaying movement-related attributes such as immigration from one country to another or wind speed and direction. Figure \ref{fig:flow} depicts an example of a flow map. (Ibid.)

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[width=9cm]{images/flow-example.jpg}
    \caption{Flow map displaying the French wine exports in 1864 \citep{minard_carte_1865}.}
    \label{fig:flow}
  \end{center}
\end{figure}

~

Users often use even a single thematic map for multiple different purposes \citep[chap.~2]{schlichtmann_visualization_2002}. For instance, a single map can be read on the \emph{overall level} (``where are the primary schools located in Helsinki metropolitan area?'') and \emph{elementary level} (``is there a primary school in Punavuori?''). Furthermore, some possible uses for a thematic map are ``what is the ratio and distribution of Finnish schools compared to Swedish schools in Helsinki'' or ``what is the spatial distribution of sizes of schools in Helsinki''. Therefore, an effective map visualization should not lock the user to any single perspective.
% passive, because emphasizing the ability ("can be read")

\subsection{Effective Thematic Maps}
\label{subsection:effectivemaps}

While the map visualizations adhere to general visualization principles, cartographers have refined the principles by specifying a set of guidelines for maps specifically. \citeauthor{koeman_het_1969} (\citeyear{koeman_het_1969} quoted by \citealt[p.~12]{kraak_cartographic_1998}) defines the guidelines for map visualization process as \emph{``How do I say what to whom''}. \emph{How} refers to the mapping methods and techniques used. \emph{What} refers to the data used and its characteristics. \emph{Whom} refers to the target audience of the visualization. \citet{kraak_cartographic_1998} complements the guidelines with \emph{``and is it effective''}, referring to the self-reflective and iterative nature of visualization.

All the elements above are important to acknowledge when creating a thematic map. While they do not provide an exact formula for determining the effectiveness of a visualization, the elements are incredibly beneficial for creating an effective visualization. Therefore, visualizers can also examine the created visualizations with the help of the elements.

\section{How Thematic Maps Are Made}
\label{section:thematicmaps}
\citet{schlichtmann_visualization_2002} describes making thematic maps as a six-step process. I present the steps below:
\begin{enumerate}
	\item Decide what is the knowledge the viewer should gain from viewing the visualization
	\item Decide on the information to be entered to the visualization
	\item Procure the data needed
	\item Procure a base with the required geometrical characteristics
	\item Select the appropriate graphic means and transcribe the information as necessary
	\item Explain the transcription in a legend
\end{enumerate}

\citet[chap.~1]{slocum_thematic_2014} present an alternative process for thematic visualization. The process consists of five steps. I present the steps below:

\begin{enumerate}
	\item Consider what the real-world distribution of the phenomenon might look like
	\item Determine the purpose of the map and its intended audience
	\item Collect data appropriate for the map's purpose
	\item Design and construct the map
	\item Determine whether users find the map useful and informative
\end{enumerate}

In practice, the processes described by \citeauthor{schlichtmann_visualization_2002} and \citeauthor{slocum_thematic_2014} concentrate on different perspectives of thematic mapping. \citeauthor{schlichtmann_visualization_2002} begins the process by defining the goal of the visualization while \citeauthor{slocum_thematic_2014} provides a data-centric approach starting with phenomenon definition. Unlike \citeauthor{schlichtmann_visualization_2002}, \citeauthor{slocum_thematic_2014} emphasize an iterative approach of the visualization. Both processes emphasize defining and designing the visualization, the actual visualization (turning the data into visual representation) being addressed in only one of the steps.

\citet[chap.~1]{slocum_thematic_2014} express their concern on the utilization of the processes defined above. According to them, it is likely that naive visualizers do not follow the steps, but take shortcuts when designing the visualizations, resulting in subpar visualizations. Therefore, visualization tools may need to nudge the visualizers towards using (one of) the processes when building a visualization.

The steps are used to produce a visual representation (graphic) of the data. Additionally, \citet{schlichtmann_visualization_2002} identifies several objectives for the resulting graphic. I present the objectives in table \ref{table:geovisualizationobjectives}.
% passive, because emphasizing the purpose of the steps

\LTcapwidth=\textwidth
\begin{longtable}{|p{3cm}|p{10cm}|}
\hline
\textbf{Name} & \textbf{Description} \\ 
\hline
\endhead
\hline
\endfoot
\endlastfoot
Clarification & Making the map clear and readable. In practice, this means that the topemes (symbols) in a map should be easily detectable and distinguishable from each other \\[0.5em]
Emphasis & Making topemes and other important characteristics of the visualization to stand out visually \\[0.5em]
Types of Entries & Having a clearly distinguishable type for each topeme. \\[0.5em]
Sets of Types & Grouping data points and symbols with similar traits in order to make them belong together visually. Ideally, the visual similarity should be related to the conceptual similarity. \\[0.5em]
Cross-Relations & Visually indicating the potential relations and similarities between different types or between entries of different types. \\[0.5em]
Local Syntax & Aligning visual properties of the topemes to prevent unintentional emphasis of single topemes. \\[0.5em]
Local Ensembles & Supporting topemes with multiple properties (such as the numbers of children and adults in an area) so that the topeme visually reflect both the individual properties and the combination of all properties. \\[0.5em]
Multilocal Ensembles & Supporting topemes with multiple geographical properties (such as spatial distribution of people)  \\[0.5em]
Addable and Non-Addable Quantities & Differentiating addable and non-addable properties. Typically absolute quantitative properties are addable while relative and qualitative properties are non-addable. Addable properties should be visualized in a way that cognitively supports addition (e.g., with sizes of elements) while non-addable quantities should be visualized without said feature (e.g., with colors.) \\[0.5em]
The Surface Illusion & Creating an illusion of surface on the map. This can be achieved for example by using illumination and shadowing. These visual traits can convey a meaning themselves and often naturally do so. \\
\hline
\caption{Map visualization objectives as per \citet{schlichtmann_visualization_2002}}
\label{table:geovisualizationobjectives}
\end{longtable}


The objectives above are important when visualizing geographical data on a map. Therefore, when creating a visualization tool, the developer needs to take those into account. Considering the objectives helps the developer to enable or encourage the visualizers to reach as many of the objectives as possible.

\chapter{Reuse in Data Visualization}
\label{chapter:reuseinvisualization}

In this chapter, I present a number of visualization reuse cases in order to demonstrate the feasibility of such approach in general. I also discuss the lack of tools and studies of software reuse in the field of geographic data visualization.

\section{Reuse Cases in Literature}

Several studies have used software reuse successfully in the field of data visualization. \citet{fekete_infovis_2004} introduces InfoVis ToolKit, a reusable library for efficient building of information visualizations. He claims that building visualizations with the toolkit is efficient, typically taking only hours. However, he does not verify this claim in any way, e.g., by comparing the efficiency to building visualizations without the toolkit.

InfoVis ToolKit consists of several different visualization components, enabling scatter plots, time series and tree maps among others. Additionally, the toolkit includes interaction components and data structures suited for visualization. According to \citeauthor{fekete_infovis_2004}, the primary benefits of the toolkit include simplifying the usage of the most common information visualization methods. Also the toolkit benefits implementation of supplementary visualization methods by providing the structure and utilities required.

\citet{heer_prefuse:_2005} reported significant benefits in the effort needed for building data visualizations using Prefuse, a reusable visualization toolkit. Using the toolkit, they were able to reduce the development time from days or weeks to minutes.

Prefuse provides a composable, modular and extendable toolkit for data visualizations. According to \citet{heer_prefuse:_2005}, it provides a highly customizable set of building blocks which can be combined and composed to create a wide variety of visualizations. However, the blocks provided do not involve functionality for efficient building of geographical visualizations.

\citet{bostock_protovis:_2009} present Protovis, a graphical toolkit for visualization. Protovis provides a data-centric approach for displaying visualizations bottom-up, favoring minimalistic graphics. In their study, \citeauthor{bostock_protovis:_2009} state that the choice of visualization tool may affect the effectiveness of visualization. According to them, the approach of Protovis discourages ``chartjunk'' and encourages building effective visualizations. However, they do no validate the claim. Nonetheless, their study indicates that Protovis benefits the visualization by providing a concise notation for building visualizations while still allowing thorough customization.

\section{Research Gap}

Currently, research on geographic or map visualization is abundant (e.g., \citealt{kraak_cartographic_1998,kraak_cartography_2011,slocum_thematic_2014,schlichtmann_visualization_2002}). Moreover, several studies report successful use of software reuse for making data visualization development more efficient \citep{heer_prefuse:_2005,bostock_protovis:_2009}. However, research on the effects of reuse on geographical visualization is scant. Additionally, few reusable web geovisualization tools exist, none of which is sufficiently high-level for enabling efficient building of effective geovisualizations. This implies that there is room for improvement in both research and implementation related to geovisualization software.

To address this shortcoming, I decided to attempt creating a reusable geovisualization tool for the web. I also evaluated the tool in order to obtain knowledge about its benefits when compared to building geographic visualizations from the beginning using lower-level tools. According to the software reuse literature (e.g., \citealt{mohagheghi_empirical_2008,boehm_managing_1999}), software reuse may reduce the needed effort dramatically for building new software. Moreover, effectiveness is one of the most important properties of a visualization \citep{kraak_cartographic_1998}. Therefore, I decided to concentrate on evaluation of the effects related to effort and visualization effectiveness.
